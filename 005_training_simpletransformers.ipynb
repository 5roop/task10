{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_set = ['Negative', 'Neutral', 'Positive']\n",
    "STR_TO_NUM = {k: i for i, k in enumerate(label_set)}\n",
    "NUM_TO_STR = {i:k for i, k in enumerate(label_set)}\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_json(\"bcs_polsent.jsonl\", orient=\"records\", lines=True)\n",
    "df[\"label\"] = df.label.apply(lambda s: STR_TO_NUM[s])\n",
    "df = df[[\"sentence\", \"label\", \"split\"]].rename(columns={\"sentence\": \"text\", \"label\":\"labels\"})\n",
    "train = df[df.split==\"train\"].drop(columns=[\"split\"])\n",
    "dev = df[df.split==\"dev\"].drop(columns=[\"split\"])\n",
    "test = df[df.split==\"test\"].drop(columns=[\"split\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions for training and evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_df, model_name):\n",
    "    if model_name == \"EMBEDDIA/crosloengual-bert\":\n",
    "        model_type = \"bert\"\n",
    "    elif model_name == \"classla/bcms-bertic\":\n",
    "        model_type = \"electra\"\n",
    "    elif model_name == \"xlm-roberta-base\":\n",
    "        model_type = \"xlmroberta\"\n",
    "    else:\n",
    "        raise AttributeError(f\"Expected either xlm-roberta-base, classla/bcms-bertic, or EMBEDDIA/crosloengual-bert, got {model_name}.\")\n",
    "\n",
    "    from simpletransformers.classification import ClassificationModel\n",
    "    import torch\n",
    "    torch.cuda.empty_cache()\n",
    "    model_args = {\n",
    "        \"num_train_epochs\": 8,\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"overwrite_output_dir\": True,\n",
    "        \"train_batch_size\": 8, \n",
    "        \"no_save\": True,\n",
    "        \"no_cache\": True,\n",
    "        \"overwrite_output_dir\": True,\n",
    "        \"save_steps\": -1,\n",
    "        \"max_seq_length\": 512,\n",
    "        \"silent\": True,\n",
    "    }\n",
    "\n",
    "    model = ClassificationModel(\n",
    "        model_type, model_name, num_labels=3, use_cuda=True, args=model_args\n",
    "    )\n",
    "    model.train_model(train_df)\n",
    "    return model\n",
    "\n",
    "\n",
    "def eval_model(model, test_df):\n",
    "    y_true_enc = test_df.labels\n",
    "    from tqdm.auto import tqdm\n",
    "    y_pred_enc = [model.predict(i)[0][0] for i in tqdm(test_df.text.values)]\n",
    "\n",
    "    y_true = [NUM_TO_STR[i] for i in y_true_enc]\n",
    "    y_pred = [NUM_TO_STR[i] for i in y_pred_enc]\n",
    "    from sklearn.metrics import f1_score\n",
    "    macroF1 = f1_score(y_true, y_pred, labels=label_set, average=\"macro\")\n",
    "\n",
    "    return {\"macroF1\": macroF1, \"y_true\": y_true, \"y_pred\": y_pred}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3126397c19ae42ceab49554a6eedffe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/peterr/anaconda3/lib/python3.8/site-packages/simpletransformers/classification/classification_model.py:459: UserWarning: use_multiprocessing automatically disabled as xlmroberta fails when using multiprocessing for feature conversion.\n",
      "  warnings.warn(\n",
      "/home/peterr/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "328a33d5c71e467c9d849e4ff5e28880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = \"xlm-roberta-base classla/bcms-bertic EMBEDDIA/crosloengual-bert\".split()\n",
    "from tqdm.auto import tqdm\n",
    "for model_name in tqdm(models):\n",
    "    model = train_model(train, model_name)\n",
    "    stats = eval_model(model, test)\n",
    "    stats[\"train_split\"] = \"train\"\n",
    "    stats[\"eval_split\"] = \"test\"\n",
    "    stats[\"model_name\"] = model_name\n",
    "    with open(\"005_results.jsonl\", \"a\") as f:\n",
    "        f.write(f\"{stats}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f6f5766036ee03d059e365a942add07f79c17033585e9357ee8157d52fe6bb9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
