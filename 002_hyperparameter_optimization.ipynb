{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook inspects optimal hyperparameters for classification models finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from numba import cuda\n",
    "cuda.select_device(0)\n",
    "cuda.close()\n",
    "cuda.select_device(0)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_set = ['Negative', 'Positive', 'Neutral']\n",
    "STR_TO_NUM = {k: i for i, k in enumerate(label_set)}\n",
    "NUM_TO_STR = {i:k for i, k in enumerate(label_set)}\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_json(\"bcs_polsent.jsonl\", orient=\"records\", lines=True)\n",
    "df[\"label\"] = df.label.apply(lambda s: STR_TO_NUM[s])\n",
    "df = df[[\"sentence\", \"label\", \"split\"]].rename(columns={\"sentence\": \"text\", \"label\":\"labels\"})\n",
    "train = df[df.split==\"train\"].drop(columns=[\"split\"])\n",
    "dev = df[df.split==\"dev\"].drop(columns=[\"split\"])\n",
    "test = df[df.split==\"test\"].drop(columns=[\"split\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/peterr/anaconda3/lib/python3.8/site-packages/simpletransformers/classification/classification_model.py:459: UserWarning: use_multiprocessing automatically disabled as xlmroberta fails when using multiprocessing for feature conversion.\n",
      "  warnings.warn(\n",
      "/home/peterr/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/peterr/anaconda3/lib/python3.8/site-packages/simpletransformers/classification/classification_model.py:459: UserWarning: use_multiprocessing automatically disabled as xlmroberta fails when using multiprocessing for feature conversion.\n",
      "  warnings.warn(\n",
      "/home/peterr/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at EMBEDDIA/crosloengual-bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at EMBEDDIA/crosloengual-bert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/peterr/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/peterr/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "Some weights of the model checkpoint at classla/bcms-bertic were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at classla/bcms-bertic and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/peterr/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/peterr/anaconda3/lib/python3.8/site-packages/simpletransformers/classification/classification_model.py:459: UserWarning: use_multiprocessing automatically disabled as xlmroberta fails when using multiprocessing for feature conversion.\n",
      "  warnings.warn(\n",
      "/home/peterr/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_model(train_df, model_name, model_type, batch_size, NUM_EPOCHS=5):\n",
    "    from simpletransformers.classification import ClassificationModel\n",
    "    model_args = {\n",
    "        \"num_train_epochs\": NUM_EPOCHS,\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"overwrite_output_dir\": True,\n",
    "        \"train_batch_size\": batch_size,\n",
    "        \"no_save\": True,\n",
    "        \"no_cache\": True,\n",
    "        \"overwrite_output_dir\": True,\n",
    "        \"save_steps\": -1,\n",
    "        \"max_seq_length\": 512,\n",
    "        \"silent\": True\n",
    "    }\n",
    "\n",
    "    model = ClassificationModel(\n",
    "        model_type, model_name,\n",
    "        num_labels = 3,\n",
    "        use_cuda = True,\n",
    "        args = model_args\n",
    "    )\n",
    "    model.train_model(train_df)\n",
    "    return model\n",
    "\n",
    "def eval_model(model, test_df):\n",
    "    y_true_enc = test_df.labels\n",
    "    y_pred_enc = model.predict(test_df.text.tolist())[0]\n",
    "\n",
    "    y_true = [NUM_TO_STR[i] for i in y_true_enc]\n",
    "    y_pred = [NUM_TO_STR[i] for i in y_pred_enc]\n",
    "    from sklearn.metrics import f1_score\n",
    "    microF1 = f1_score(y_true, y_pred, labels=[\"Positive\", \"Neutral\", \"Negative\"], average =\"micro\")\n",
    "    macroF1 = f1_score(y_true, y_pred, labels=[\"Positive\", \"Neutral\", \"Negative\"], average =\"macro\")\n",
    "\n",
    "    return {\"microF1\": microF1, \n",
    "            \"macroF1\": macroF1,\n",
    "            \"y_true\": y_true,\n",
    "            \"y_pred\": y_pred}\n",
    "\n",
    "\n",
    "for epoch in [3, 5, 9, 15]:\n",
    "    for batch in [8, 16]:\n",
    "        for modeltype, modelname in zip(\n",
    "            [\"xlmroberta\", \"xlmroberta\", \"bert\", \"electra\"],\n",
    "            [\"xlm-roberta-base\", \"xlm-roberta-large\", \"EMBEDDIA/crosloengual-bert\", \"classla/bcms-bertic\"]):\n",
    "            model = train_model(train, modelname, modeltype,batch,NUM_EPOCHS=epoch)\n",
    "            stats = eval_model(model, dev)\n",
    "            with open(\"results3.csv\", \"a\") as f:\n",
    "                f.write(f\"{modelname},{modeltype},{epoch},{batch},{stats['macroF1']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'microF1': 0.78,\n",
       " 'macroF1': 0.7610070962584262,\n",
       " 'y_true': ['Neutral',\n",
       "  'Positive',\n",
       "  'Neutral',\n",
       "  'Positive',\n",
       "  'Neutral',\n",
       "  'Neutral',\n",
       "  'Positive',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Neutral',\n",
       "  'Neutral',\n",
       "  'Neutral',\n",
       "  'Neutral',\n",
       "  'Negative',\n",
       "  'Neutral',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Positive',\n",
       "  'Negative',\n",
       "  'Positive',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Positive',\n",
       "  'Negative',\n",
       "  'Neutral',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Positive',\n",
       "  'Positive',\n",
       "  'Neutral',\n",
       "  'Negative',\n",
       "  'Positive',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Positive',\n",
       "  'Positive',\n",
       "  'Negative',\n",
       "  'Positive',\n",
       "  'Negative',\n",
       "  'Positive',\n",
       "  'Negative',\n",
       "  'Positive',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Neutral',\n",
       "  'Neutral',\n",
       "  'Neutral',\n",
       "  'Negative',\n",
       "  'Neutral',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Neutral',\n",
       "  'Negative',\n",
       "  'Neutral',\n",
       "  'Positive',\n",
       "  'Positive',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Neutral',\n",
       "  'Positive',\n",
       "  'Positive',\n",
       "  'Negative',\n",
       "  'Neutral',\n",
       "  'Positive',\n",
       "  'Negative',\n",
       "  'Positive',\n",
       "  'Neutral',\n",
       "  'Neutral',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Neutral',\n",
       "  'Neutral',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Neutral',\n",
       "  'Neutral',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Positive',\n",
       "  'Negative',\n",
       "  'Neutral',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Positive',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Neutral',\n",
       "  'Negative',\n",
       "  'Neutral',\n",
       "  'Neutral',\n",
       "  'Negative',\n",
       "  'Neutral',\n",
       "  'Positive',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Neutral',\n",
       "  'Positive',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Positive',\n",
       "  'Negative',\n",
       "  'Positive',\n",
       "  'Neutral',\n",
       "  'Negative',\n",
       "  'Neutral',\n",
       "  'Negative',\n",
       "  'Neutral',\n",
       "  'Positive',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Neutral',\n",
       "  'Neutral',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Neutral',\n",
       "  'Neutral',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Neutral',\n",
       "  'Negative',\n",
       "  'Neutral',\n",
       "  'Positive',\n",
       "  'Neutral',\n",
       "  'Positive',\n",
       "  'Negative',\n",
       "  'Positive',\n",
       "  'Neutral',\n",
       "  'Positive',\n",
       "  'Positive',\n",
       "  'Negative'],\n",
       " 'y_pred': ['Neutral',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Neutral',\n",
       "  'Positive',\n",
       "  'Positive',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Neutral',\n",
       "  'Neutral',\n",
       "  'Negative',\n",
       "  'Positive',\n",
       "  'Neutral',\n",
       "  'Positive',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Neutral',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Positive',\n",
       "  'Negative',\n",
       "  'Positive',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Positive',\n",
       "  'Negative',\n",
       "  'Neutral',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Positive',\n",
       "  'Neutral',\n",
       "  'Neutral',\n",
       "  'Neutral',\n",
       "  'Positive',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Positive',\n",
       "  'Positive',\n",
       "  'Positive',\n",
       "  'Neutral',\n",
       "  'Positive',\n",
       "  'Positive',\n",
       "  'Positive',\n",
       "  'Negative',\n",
       "  'Neutral',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Neutral',\n",
       "  'Neutral',\n",
       "  'Negative',\n",
       "  'Neutral',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Neutral',\n",
       "  'Neutral',\n",
       "  'Negative',\n",
       "  'Neutral',\n",
       "  'Neutral',\n",
       "  'Positive',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Neutral',\n",
       "  'Positive',\n",
       "  'Negative',\n",
       "  'Neutral',\n",
       "  'Negative',\n",
       "  'Positive',\n",
       "  'Negative',\n",
       "  'Positive',\n",
       "  'Neutral',\n",
       "  'Neutral',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Neutral',\n",
       "  'Neutral',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Neutral',\n",
       "  'Neutral',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Positive',\n",
       "  'Negative',\n",
       "  'Neutral',\n",
       "  'Neutral',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Neutral',\n",
       "  'Negative',\n",
       "  'Neutral',\n",
       "  'Neutral',\n",
       "  'Negative',\n",
       "  'Neutral',\n",
       "  'Neutral',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Positive',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Neutral',\n",
       "  'Neutral',\n",
       "  'Neutral',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Positive',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Neutral',\n",
       "  'Neutral',\n",
       "  'Neutral',\n",
       "  'Positive',\n",
       "  'Negative',\n",
       "  'Neutral',\n",
       "  'Neutral',\n",
       "  'Neutral',\n",
       "  'Negative',\n",
       "  'Negative',\n",
       "  'Neutral',\n",
       "  'Negative',\n",
       "  'Positive',\n",
       "  'Positive',\n",
       "  'Neutral',\n",
       "  'Positive',\n",
       "  'Negative',\n",
       "  'Positive',\n",
       "  'Neutral',\n",
       "  'Positive',\n",
       "  'Positive',\n",
       "  'Negative']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(model, dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f6f5766036ee03d059e365a942add07f79c17033585e9357ee8157d52fe6bb9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
